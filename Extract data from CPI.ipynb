{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:13:20.248940Z",
     "start_time": "2019-09-23T08:13:20.244093Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:21:43.744691Z",
     "start_time": "2019-06-17T07:21:43.739836Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTraceList():\n",
    "    tracelist = os.listdir(\"./Traces\")\n",
    "    tracelist.remove('.DS_Store')\n",
    "    return tracelist\n",
    "getTraceList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:16:39.105179Z",
     "start_time": "2019-06-17T07:16:39.101321Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(match, logdata):\n",
    "    timestamp, timezone, status, message = match.groups()\n",
    "    logdata = logdata.append({\n",
    "        'datetime': timestamp, \n",
    "        'timezone': timezone, \n",
    "        'status': status, \n",
    "        'log': message\n",
    "    }, ignore_index=True)\n",
    "    return logdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:20:14.111869Z",
     "start_time": "2019-06-17T07:18:58.531322Z"
    }
   },
   "outputs": [],
   "source": [
    "def processTraces():\n",
    "    tracelist = getTraceList()\n",
    "    logdata = pd.DataFrame(columns=['datetime', 'timezone', 'status','log'])\n",
    "    pbar = tqdm(total=len(tracelist), leave=False)\n",
    "    pbar.set_description(desc='Processing log files', refresh=True)\n",
    "    \n",
    "    for log in tracelist:\n",
    "        with open(\"./Traces/\" + log, 'r') as file:  \n",
    "            logfile = file.read()\n",
    "            logEntry_re = re.compile(r\"^(?:(\\d{4}(?: \\d{2}){3}:\\d{2}:\\d{2})#(\\+\\d{2})#(\\w+?)#((?:.|\\n)+?)\\|)\",\\\n",
    "                                       re.MULTILINE)\n",
    "            for match in logEntry_re.finditer(logfile):\n",
    "                logdata = process(match, logdata)\n",
    "        pbar.update(1)\n",
    "    return logdata\n",
    "\n",
    "logs = processTraces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:25:11.285650Z",
     "start_time": "2019-06-17T07:25:11.228512Z"
    }
   },
   "outputs": [],
   "source": [
    "logs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OData Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_user_pass = {}\n",
    "with open('url_user_pass.json') as json_file:\n",
    "    url_user_pass = json.load(json_file)\n",
    "    url_user_pass = url_user_pass.get(\"ExtractData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:39:06.130813Z",
     "start_time": "2019-07-31T09:39:06.076813Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:54:51.738807Z",
     "start_time": "2019-06-17T07:54:51.735814Z"
    }
   },
   "outputs": [],
   "source": [
    "host = url_user_pass.get(\"PostNL\").get(\"qashost\")\n",
    "# url = url_user_pass.get(\"QforIT\").get(\"host\")\n",
    "# url = url_user_pass.get(\"other\").get(\"url2\")\n",
    "\n",
    "url='https://'+host+url_user_pass.get(\"PostNL\").get(\"baseurl\")\n",
    "\n",
    "baseurl, query, top, skip, filters = url,'MessageProcessingLogs?$inlinecount=allpages&$format=json&$orderby=LogEnd%20desc','&$top=','&$skip=','&$filter=LogEnd%20ge%20datetime%272019-06-17T12:20:00%27%20and%20LogStart%20le%20datetime%272019-06-17T07:38:34%27%20and%20IntegrationArtifact%2FType%20eq%20%27INTEGRATION_FLOW%27'\n",
    "fullurl = \"https://\"+host+url_user_pass.get(\"PostNL\").get(\"baseurl\"),\"MessageProcessingLogs?$inlinecount=allpages&$format=json\"\n",
    "MSG_DETAIL_URL = 'https://'+host+url_user_pass.get(\"PostNL\").get(\"commandurl\")+\"com.sap.it.op.tmn.commands.dashboard.webui.MplDetailCommand?messageGuid='\n",
    "\n",
    "session = requests.Session()\n",
    "session.auth = (url_user_pass.get(\"suser\"), url_user_pass.get(\"password\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:54:52.720059Z",
     "start_time": "2019-06-17T07:54:52.244981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create instance of OData client\n",
    "SAPCPI = pyodata.Client(fullurl, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:13:20.262176Z",
     "start_time": "2019-09-23T08:13:20.252837Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re\n",
    "from tqdm.autonotebook import tqdm\n",
    "import requests\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import io\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "from multiprocessing.dummy import Pool\n",
    "import urllib3\n",
    "from requests.adapters import HTTPAdapter\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:13:20.273631Z",
     "start_time": "2019-09-23T08:13:20.265648Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "host = url_user_pass.get(\"PostNL\").get(\"qashost\")\n",
    "# host='p0204' #QforIT tenant\n",
    "\n",
    "session = requests.Session()\n",
    "session.auth = (url_user_pass.get(\"suser\"), url_user_pass.get(\"password\"))\n",
    "session.mount('https://', HTTPAdapter(pool_connections=5, pool_maxsize=20))\n",
    "def getCount():\n",
    "    counturl = \"\"\n",
    "    print(\"removed url\")\n",
    "    r = session.get(str(counturl))\n",
    "    count = r.content\n",
    "    bcount = int(str(count)[2:8])\n",
    "    return bcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:13:20.285161Z",
     "start_time": "2019-09-23T08:13:20.278369Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def requesturl(top,skip, url):\n",
    "    requesturl = url.format(top,skip)\n",
    "    return requesturl\n",
    "\n",
    "def request(top,skip, url):\n",
    "    req = requesturl(top,skip, str(url))\n",
    "    res = session.get(req)\n",
    "    contents = res.content.decode('utf8')\n",
    "    data = json.loads(contents)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:28:36.795323Z",
     "start_time": "2019-08-09T06:28:36.792158Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MSG_DETAIL_URL = ''\n",
    "\n",
    "# # MessageGuidURL = \"\"\n",
    "# def getMessageText(guids):\n",
    "#     session = requests.Session()\n",
    "#     session.auth = ('','')\n",
    "\n",
    "#     def req_split(GUID):\n",
    "#         res = session.get(MSG_DETAIL_URL + GUID)\n",
    "#         contents = res.content.decode('utf8')\n",
    "#         root = ET.fromstring(contents)\n",
    "#         tag = {}\n",
    "#         tag[\"GUID\"] = GUID\n",
    "#         tag[\"MSG\"] = root.findtext(\".//mplData\")\n",
    "        \n",
    "#         pbar.update(1)\n",
    "#         return tag\n",
    "            \n",
    "#     df = pd.DataFrame(columns=['GUID','MSG'])  \n",
    "    \n",
    "#     pbar = tqdm(total=len(guids), leave=False)\n",
    "#     pbar.set_description(desc='Downloading log entries text', refresh=True)\n",
    "\n",
    "    \n",
    "#     with Pool(45) as p:\n",
    "#         pm = p.imap_unordered(req_split,guids)\n",
    "#         pm = [i for i in pm if i]\n",
    "#         df = df.append(pm, ignore_index=True)\n",
    "        \n",
    "#     pbar.close()\n",
    "#     return df    \n",
    "\n",
    "# def getmsgfilename(date1,date2):\n",
    "#     filename = \"log msg\" + str(date1) + \" to \" + str(date2) + \".csv\"\n",
    "#     return filename\n",
    "        \n",
    "# def getDetails(guids, date, newmaxdate):\n",
    "#     df = getMessageText(guids)\n",
    "#     filename = getmsgfilename(date, newmaxdate)\n",
    "#     df.to_csv(filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:28:37.197333Z",
     "start_time": "2019-08-09T06:28:37.192695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# def processMsg(df, data):\n",
    "#     for msg in data.get('d').get('results'):\n",
    "#         dat = dict({\n",
    "#             'MessageGuid': msg.get('MessageGuid'),\n",
    "#             'CorrelationId': msg.get('CorrelationId'),\n",
    "#             'ApplicationMessageId': msg.get('ApplicationMessageId'),\n",
    "#             'ApplicationMessageType': msg.get('ApplicationMessageType'),\n",
    "#             'LogStart': msg.get('LogStart'),\n",
    "#             'LogEnd': msg.get('LogEnd'),\n",
    "#             'Sender': msg.get('Sender'),\n",
    "#             'Receiver': msg.get('Receiver'),\n",
    "#             'IntegrationFlowName': msg.get('IntegrationFlowName'),\n",
    "#             'Status': msg.get('Status'),\n",
    "#             'LogLevel': msg.get('LogLevel'),\n",
    "#             'CustomStatus': msg.get('CustomStatus'),\n",
    "#             'TransactionId': msg.get('TransactionId'),\n",
    "#             'PreviousComponentName': msg.get('PreviousComponentName') \n",
    "#                   })\n",
    "#         df = df.append(dat, ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "# def convertdatetime(x):\n",
    "#     regexstring = r\"\\/\\w{4}\\((\\d*?)\\d{3}\\)\\/\"\n",
    "#     res = re.search(regexstring, x)\n",
    "#     if res:\n",
    "#         dt = datetime.datetime.fromtimestamp(int(res.group(1))).isoformat()\n",
    "#         return dt\n",
    "#     return None\n",
    "        \n",
    "# def download_logs(lastdate = None):\n",
    "#     skipamount = 1000\n",
    "    \n",
    "#     df = pd.DataFrame(columns=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType','LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel','CustomStatus','TransactionId','PreviousComponentName'])\n",
    "    \n",
    "#     url = \"\"\n",
    "#     if lastdate != None:\n",
    "#         url = url + \"&$filter=LogEnd ge datetime'\"+lastdate+\"'\"\n",
    "    \n",
    "#     data = request(skipamount,skipamount,url)\n",
    "#     count = data.get('d').get('__count')\n",
    "    \n",
    "#     requestcount = math.ceil(int(count)/skipamount)\n",
    "    \n",
    "#     pbar = tqdm(total=requestcount*skipamount, leave=False)\n",
    "#     pbar.set_description(desc='Downloading log entries', refresh=True)\n",
    "#     pbar.update(skipamount)\n",
    "#     for i in range(0,requestcount):\n",
    "#         data = request(skipamount,i*skipamount,url)\n",
    "#         df = processMsg(df, data)\n",
    "#         pbar.update(skipamount)\n",
    "#     dfcount = df.shape[0]\n",
    "#     if dfcount > 0:\n",
    "#         df['LogStart'] = df.apply(lambda row: convertdatetime(row['LogStart']), axis=1).apply(pd.to_datetime)\n",
    "#         df['LogEnd'] = df.apply(lambda row: convertdatetime(row['LogEnd']), axis=1).apply(pd.to_datetime)\n",
    "    \n",
    "#     pbar.close()\n",
    "#     return df\n",
    "\n",
    "# def getMaxDate(df):\n",
    "#     start = df.LogStart.max()\n",
    "#     end = df.LogEnd.max()\n",
    "#     if start >= end:\n",
    "#         return start\n",
    "#     elif end > start:\n",
    "#         return end\n",
    "\n",
    "# def getLogs():\n",
    "#     date = readfromfile()\n",
    "#     date = parser.parse(str(date))\n",
    "#     date = date.isoformat()\n",
    "#     df = download_logs(date)\n",
    "#     dfcount = df.shape[0]\n",
    "#     if dfcount > 0:\n",
    "#         newmaxdate = getMaxDate(df)\n",
    "#         filename = getfilename(df)\n",
    "#         writetofile(newmaxdate)\n",
    "#         df.to_csv(filename)\n",
    "#         guids = df['MessageGuid'].unique()\n",
    "#         return guids, date, newmaxdate\n",
    "#     else:\n",
    "#         return [], None, None\n",
    "    \n",
    "\n",
    "# def writetofile(date):\n",
    "#     with open('latestdate.txt', 'w') as file:  \n",
    "#         file.write(str(date))\n",
    "        \n",
    "# def readfromfile():\n",
    "#     with open('latestdate.txt', 'r') as file:  \n",
    "#         data = file.read() \n",
    "#         return data\n",
    "    \n",
    "# def getfilename(df):\n",
    "#     olddate = readfromfile()\n",
    "#     newdate = parser.parse(str(getMaxDate(df))).isoformat()\n",
    "#     filename = \"logs\" + str(olddate) + \" to \" + str(newdate) + \".csv\"\n",
    "#     return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:29:48.211665Z",
     "start_time": "2019-08-09T06:29:48.191241Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def processMsg(stack, data):\n",
    "    for msg in data.get('d').get('results'):\n",
    "        dat = dict({\n",
    "            'MessageGuid': msg.get('MessageGuid'),\n",
    "            'CorrelationId': msg.get('CorrelationId'),\n",
    "            'ApplicationMessageId': msg.get('ApplicationMessageId'),\n",
    "            'ApplicationMessageType': msg.get('ApplicationMessageType'),\n",
    "            'LogStart': convertdatetime(msg.get('LogStart')),\n",
    "            'LogEnd': convertdatetime(msg.get('LogEnd')),\n",
    "            'Sender': msg.get('Sender'),\n",
    "            'Receiver': msg.get('Receiver'),\n",
    "            'IntegrationFlowName': msg.get('IntegrationFlowName'),\n",
    "            'Status': msg.get('Status'),\n",
    "            'LogLevel': msg.get('LogLevel'),\n",
    "            'CustomStatus': msg.get('CustomStatus'),\n",
    "            'TransactionId': msg.get('TransactionId'),\n",
    "            'PreviousComponentName': msg.get('PreviousComponentName') \n",
    "        })\n",
    "        stack.append(dat)\n",
    "    return stack\n",
    "\n",
    "def convertdatetime(x):\n",
    "    regexstring = r\"\\/\\w{4}\\((\\d*?)\\d{3}\\)\\/\"\n",
    "    res = re.search(regexstring, x)\n",
    "    if res:\n",
    "        dt = datetime.fromtimestamp(int(res.group(1))).isoformat()\n",
    "        return dt\n",
    "    return None\n",
    "        \n",
    "def download_logs(lastdate = None):\n",
    "    skipamount = 1000\n",
    "    \n",
    "    stack = []#pd.DataFrame(columns=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType','LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel','CustomStatus','TransactionId','PreviousComponentName'])\n",
    "    \n",
    "    url = \"\"\n",
    "    if lastdate != None:\n",
    "        url = url + \"&$filter=LogEnd ge datetime%27\"+lastdate[:-3]+\"%27\"\n",
    "    data = request(skipamount,skipamount,url)\n",
    "    count = data.get('d').get('__count')\n",
    "    \n",
    "    requestcount = math.ceil(int(count)/skipamount)\n",
    "    \n",
    "    pbar = tqdm(total=requestcount*skipamount, leave=False)\n",
    "    pbar.set_description(desc='Downloading log entries', refresh=True)\n",
    "    pbar.update(skipamount)\n",
    "    for i in range(0,requestcount):\n",
    "        data = request(skipamount,i*skipamount,url)\n",
    "        stack = processMsg(stack, data)\n",
    "        pbar.update(skipamount)\n",
    "#     dfcount = df.shape[0]\n",
    "#     if dfcount > 0:\n",
    "#         df['LogStart'] = df.apply(lambda row: convertdatetime(row['LogStart']), axis=1).apply(pd.to_datetime)\n",
    "#         df['LogEnd'] = df.apply(lambda row: convertdatetime(row['LogEnd']), axis=1).apply(pd.to_datetime)\n",
    "    \n",
    "    pbar.close()\n",
    "    return stack\n",
    "\n",
    "def getMaxDate(df):\n",
    "    start = df.LogStart.max()\n",
    "    end = df.LogEnd.max()\n",
    "    if start >= end:\n",
    "        return start\n",
    "    elif end > start:\n",
    "        return end\n",
    "\n",
    "def getLogs():\n",
    "#     date = readfromfile()\n",
    "    date = datetime.today() - timedelta(hours=3, minutes=0)\n",
    "    date = parser.parse(str(date))\n",
    "    date = date.isoformat()\n",
    "    df = download_logs(date)\n",
    "    df = pd.DataFrame(df, columns=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType','LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel','CustomStatus','TransactionId','PreviousComponentName'])\n",
    "    dfcount = df.shape[0]\n",
    "    if dfcount > 0:\n",
    "        newmaxdate = getMaxDate(df)\n",
    "        filename = getfilename(df)\n",
    "        writetofile(newmaxdate)\n",
    "        df.to_csv(filename)\n",
    "        guids = df['MessageGuid'].unique()\n",
    "        return guids, date, newmaxdate\n",
    "    else:\n",
    "        return [], None, None\n",
    "    \n",
    "\n",
    "def writetofile(date):\n",
    "    with open('latestdate.txt', 'w') as file:  \n",
    "        file.write(str(date))\n",
    "        \n",
    "def readfromfile():\n",
    "    with open('latestdate.txt', 'r') as file:  \n",
    "        data = file.read() \n",
    "        return data\n",
    "    \n",
    "def getfilename(df):\n",
    "    olddate = readfromfile()\n",
    "    newdate = parser.parse(str(getMaxDate(df))).isoformat()\n",
    "    filename = \"logs\" + str(olddate) + \" to \" + str(newdate) + \".csv\"\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:29:48.711168Z",
     "start_time": "2019-08-09T06:29:48.706096Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def processMsg(msg):\n",
    "#     if None:\n",
    "#         return dict({})\n",
    "#     dat = dict({\n",
    "#         'MessageGuid': msg.get('MessageGuid',\"\"),\n",
    "#         'CorrelationId': msg.get('CorrelationId',\"\"),\n",
    "#         'ApplicationMessageId': msg.get('ApplicationMessageId',\"\"),\n",
    "#         'ApplicationMessageType': msg.get('ApplicationMessageType',\"\"),\n",
    "#         'LogStart': msg.get('LogStart',\"\"),\n",
    "#         'LogEnd': msg.get('LogEnd',\"\"),\n",
    "#         'Sender': msg.get('Sender',\"\"),\n",
    "#         'Receiver': msg.get('Receiver',\"\"),\n",
    "#         'IntegrationFlowName': msg.get('IntegrationFlowName',\"\"),\n",
    "#         'Status': msg.get('Status',\"\"),\n",
    "#         'LogLevel': msg.get('LogLevel',\"\"),\n",
    "#         'CustomStatus': msg.get('CustomStatus',\"\"),\n",
    "#         'TransactionId': msg.get('TransactionId',\"\"),\n",
    "#         'PreviousComponentName': msg.get('PreviousComponentName',\"\") \n",
    "#     })\n",
    "#     print(dat)\n",
    "#     return dat\n",
    "\n",
    "# def convertdatetime(x):\n",
    "#     regexstring = r\"\\/\\w{4}\\((\\d*?)\\d{3}\\)\\/\"\n",
    "#     res = re.search(regexstring, x)\n",
    "#     if res:\n",
    "#         dt = datetime.datetime.fromtimestamp(int(res.group(1))).isoformat()\n",
    "#         return dt\n",
    "#     return None\n",
    "\n",
    "# def download_logs(lastdate = None):\n",
    "#     skipamount = 1000\n",
    "\n",
    "#     requestlist = []\n",
    "\n",
    "#     url = \"\"\n",
    "#     if lastdate != None:\n",
    "#         url = url + \"&$filter=LogEnd ge datetime'\"+lastdate+\"'\"\n",
    "    \n",
    "#     data = request(skipamount,skipamount,url)\n",
    "#     count = data.get('d').get('__count')\n",
    "    \n",
    "#     requestcount = math.ceil(int(count)/skipamount)\n",
    "    \n",
    "#     stack = []\n",
    "#     pbar = tqdm(total=requestcount*skipamount, leave=False)\n",
    "#     pbar.set_description(desc='Downloading log entries', refresh=True)\n",
    "\n",
    "#     for i in range(0,requestcount*skipamount, skipamount):\n",
    "#         requestlist.append((i,i+skipamount))\n",
    "\n",
    "#     def req_split(localset):\n",
    "#         data = request(localset[0],localset[1], url)\n",
    "#         data = data.get('d', None)\n",
    "#         if data != None:\n",
    "#             data = data.get('results', None)\n",
    "#             with Pool(50) as p:\n",
    "#                 pm = p.imap_unordered(processMsg,data)\n",
    "#                 pm = [i for i in pm if i]\n",
    "#                 print(pm)\n",
    "#                 stack.append(pm)\n",
    "#             pbar.update(skipamount)\n",
    "#             print(stack)\n",
    "#             return stack\n",
    "#         else:\n",
    "#             pbar.update(skipamount)\n",
    "#             return []\n",
    "# #         msgs = processMsg([], data)\n",
    "# #         return msgs\n",
    "\n",
    "\n",
    "#     with Pool(50) as p:\n",
    "#         pm = p.imap_unordered(req_split,requestlist)\n",
    "#         pm = [i for i in pm if i]\n",
    "#         print(pm)\n",
    "#         stack.append(pm)\n",
    "#     pbar.close()\n",
    "#     return stack\n",
    "\n",
    "# def getMaxDate(df):\n",
    "#     start = df.LogStart.max()\n",
    "#     end = df.LogEnd.max()\n",
    "#     if start >= end:\n",
    "#         return start\n",
    "#     elif end > start:\n",
    "#         return end\n",
    "\n",
    "# def getLogs():\n",
    "#     date = parser.parse('2019-07-31T08:23:00')\n",
    "#     date = date.isoformat()\n",
    "# #     try:\n",
    "# #         date = readfromfile()\n",
    "# #         date = parser.parse(str(date))\n",
    "# #         date = date.isoformat()\n",
    "# #         stack = download_logs()\n",
    "#     stack = download_logs(date)\n",
    "# #     except:\n",
    "# #         stack = download_logs()\n",
    "#     dfcount = len(stack)\n",
    "#     if dfcount > 0:\n",
    "#         dflist = []\n",
    "#         for i in stack:\n",
    "#             if len(i) > 0:\n",
    "#                 print(i)\n",
    "#                 df = pd.DataFrame(i, columns=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType','LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel','CustomStatus','TransactionId','PreviousComponentName'])\n",
    "#                 df['LogStart'] = df.apply(lambda row: convertdatetime(row['LogStart']), axis=1).apply(pd.to_datetime)\n",
    "#                 df['LogEnd'] = df.apply(lambda row: convertdatetime(row['LogEnd']), axis=1).apply(pd.to_datetime)\n",
    "#         #         newmaxdate = getMaxDate(df)\n",
    "#         #         filename = getfilename(df)\n",
    "#         #         writetofile(newmaxdate)\n",
    "#         #         df.to_csv(filename)\n",
    "#                 dflist.append(df)\n",
    "#         df = pd.concat(dflist, keys=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType','LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel','CustomStatus','TransactionId','PreviousComponentName'])\n",
    "#         guids = df['MessageGuid'].unique()\n",
    "#         df.to_csv(\"test\")\n",
    "#         return guids, date, newmaxdate\n",
    "#     else:\n",
    "#         return [], None, None\n",
    "    \n",
    "\n",
    "# def writetofile(date):\n",
    "#     with open('latestdate.txt', 'w+') as file:  \n",
    "#         file.write(str(date))\n",
    "        \n",
    "# def readfromfile():\n",
    "#     with open('latestdate.txt', 'r') as file:  \n",
    "#         data = file.read() \n",
    "#         return data\n",
    "    \n",
    "# def getfilename(df):\n",
    "#     olddate = readfromfile()\n",
    "#     newdate = parser.parse(str(getMaxDate(df))).isoformat()\n",
    "#     filename = \"logs\" + str(olddate) + \" to \" + str(newdate) + \".csv\"\n",
    "#     return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:29:49.137004Z",
     "start_time": "2019-08-09T06:29:49.132001Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def processMsg(msg):\n",
    "#     if None:\n",
    "#         return dict({})\n",
    "#     dat = dict({\n",
    "#         'MessageGuid': msg.get('MessageGuid',\"\"),\n",
    "#         'CorrelationId': msg.get('CorrelationId',\"\"),\n",
    "#         'ApplicationMessageId': msg.get('ApplicationMessageId',\"\"),\n",
    "#         'ApplicationMessageType': msg.get('ApplicationMessageType',\"\"),\n",
    "#         'LogStart': msg.get('LogStart',\"\"),\n",
    "#         'LogEnd': msg.get('LogEnd',\"\"),\n",
    "#         'Sender': msg.get('Sender',\"\"),\n",
    "#         'Receiver': msg.get('Receiver',\"\"),\n",
    "#         'IntegrationFlowName': msg.get('IntegrationFlowName',\"\"),\n",
    "#         'Status': msg.get('Status',\"\"),\n",
    "#         'LogLevel': msg.get('LogLevel',\"\"),\n",
    "#         'CustomStatus': msg.get('CustomStatus',\"\"),\n",
    "#         'TransactionId': msg.get('TransactionId',\"\"),\n",
    "#         'PreviousComponentName': msg.get('PreviousComponentName',\"\") \n",
    "#     })\n",
    "#     print(dat)\n",
    "#     return dat\n",
    "\n",
    "# def convertdatetime(x):\n",
    "#     regexstring = r\"\\/\\w{4}\\((\\d*?)\\d{3}\\)\\/\"\n",
    "#     res = re.search(regexstring, x)\n",
    "#     if res:\n",
    "#         dt = datetime.datetime.fromtimestamp(int(res.group(1))).isoformat()\n",
    "#         return dt\n",
    "#     return None\n",
    "\n",
    "# def download_logs(lastdate = None):\n",
    "#     skipamount = 1000\n",
    "\n",
    "#     requestlist = []\n",
    "\n",
    "#     url = \"\"\n",
    "#     if lastdate != None:\n",
    "#         url = url + \"&$filter=LogEnd ge datetime'\"+lastdate+\"'\"\n",
    "    \n",
    "#     data = request(skipamount,skipamount,url)\n",
    "#     count = data.get('d').get('__count')\n",
    "    \n",
    "#     requestcount = math.ceil(int(count)/skipamount)\n",
    "    \n",
    "#     stack = []\n",
    "#     pbar = tqdm(total=requestcount*skipamount, leave=False)\n",
    "#     pbar.set_description(desc='Downloading log entries', refresh=True)\n",
    "\n",
    "#     for i in range(0,requestcount*skipamount, skipamount):\n",
    "#         requestlist.append((i,i+skipamount))\n",
    "        \n",
    "#     def savejson(data):\n",
    "#         with open(\"dump1.json\", 'a') as file:\n",
    "#             json.dump(data, file)\n",
    "            \n",
    "#     def req_split(localset):\n",
    "#         data = request(localset[0],localset[1], url)\n",
    "#         data = data.get('d', None)\n",
    "#         if data != None:\n",
    "#             data = data.get('results', None)\n",
    "#             with Pool(50) as p:\n",
    "#                 pm = p.imap_unordered(processMsg,data)\n",
    "#                 pm = [i for i in pm if i]\n",
    "#                 stack.append(pm)\n",
    "#             pbar.update(skipamount)\n",
    "#             return stack\n",
    "#         else:\n",
    "#             pbar.update(skipamount)\n",
    "#             return []\n",
    "\n",
    "\n",
    "#     with Pool(50) as p:\n",
    "#         pm = p.imap_unordered(req_split,requestlist)\n",
    "#         pm = [i for i in pm if i]\n",
    "#         savejson(pm)\n",
    "#     pbar.close()\n",
    "#     return stack\n",
    "\n",
    "# def getMaxDate(df):\n",
    "#     start = df.LogStart.max()\n",
    "#     end = df.LogEnd.max()\n",
    "#     if start >= end:\n",
    "#         return start\n",
    "#     elif end > start:\n",
    "#         return end\n",
    "\n",
    "# def getLogs():\n",
    "#     date = parser.parse('2019-07-30T08:23:00')\n",
    "#     date = date.isoformat()\n",
    "#     stack = download_logs(date)\n",
    "#     dfcount = len(stack)\n",
    "#     if dfcount > 0:\n",
    "        \n",
    "#         return guids, date, newmaxdate\n",
    "#     else:\n",
    "#         return [], None, None\n",
    "    \n",
    "\n",
    "# def writetofile(date):\n",
    "#     with open('latestdate.txt', 'w+') as file:  \n",
    "#         file.write(str(date))\n",
    "        \n",
    "# def readfromfile():\n",
    "#     with open('latestdate.txt', 'r') as file:  \n",
    "#         data = file.read() \n",
    "#         return data\n",
    "    \n",
    "# def getfilename(df):\n",
    "#     olddate = readfromfile()\n",
    "#     newdate = parser.parse(str(getMaxDate(df))).isoformat()\n",
    "#     filename = \"logs\" + str(olddate) + \" to \" + str(newdate) + \".csv\"\n",
    "#     return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:29:49.431924Z",
     "start_time": "2019-08-09T06:29:49.422587Z"
    }
   },
   "outputs": [],
   "source": [
    "MSG_DETAIL_URL = ''\n",
    "\n",
    "# MessageGuidURL = \"\"\n",
    "def getMessageText(guids):\n",
    "    session = requests.Session()\n",
    "    session.auth = ('', '')\n",
    "    \n",
    "    stack = []\n",
    "    \n",
    "    def req_split(GUID):\n",
    "        res = session.get(MSG_DETAIL_URL + GUID)\n",
    "        contents = res.content.decode('utf8')\n",
    "        root = ET.fromstring(contents)\n",
    "        tag = (GUID, root.findtext(\".//mplData\"))\n",
    "        \n",
    "        pbar.update(1)\n",
    "        stack.append(tag) \n",
    "    \n",
    "    pbar = tqdm(total=len(guids), leave=False)\n",
    "    pbar.set_description(desc='Downloading log entries text', refresh=True)\n",
    "    \n",
    "    with Pool(49) as p:\n",
    "        pm = p.imap_unordered(req_split,guids)\n",
    "        pm = [i for i in pm if i]\n",
    "#         stack.append(pm)\n",
    "    \n",
    "#     with open('dump.txt', 'w') as file:  \n",
    "#         file.write(str(stack))\n",
    "    \n",
    "    pbar.close()\n",
    "    return stack    \n",
    "\n",
    "def getmsgfilename(date1,date2):\n",
    "    filename = \"log msg\" + str(date1) + \" to \" + str(date2) + \".csv\"\n",
    "    return filename\n",
    "        \n",
    "def getDetails(guids, date, newmaxdate):\n",
    "    stack = getMessageText(guids)\n",
    "    dt = []\n",
    "    count = 0\n",
    "    for i in stack:\n",
    "        dt.append((i))\n",
    "    df = pd.DataFrame(dt, columns=['GUID','MSG'])\n",
    "    filename = getmsgfilename(date, newmaxdate)\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:31:33.794702Z",
     "start_time": "2019-08-09T06:29:49.871610Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14636), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# waittime_in_sec = 60*60\n",
    "# while True:\n",
    "guids, date, newmaxdate = getLogs()\n",
    "if len(guids) > 0: \n",
    "    getDetails(guids, date, newmaxdate)\n",
    "#     pbar = tqdm(total=waittime_in_sec, leave=False)\n",
    "#     pbar.set_description(desc='Waiting', refresh=True)\n",
    "#     for i in range(0,waittime_in_sec):\n",
    "#         time.sleep(1)\n",
    "#         pbar.update(1)\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T06:19:36.888657Z",
     "start_time": "2019-08-09T06:19:36.885273Z"
    }
   },
   "outputs": [],
   "source": [
    "# def processMsg(msg):\n",
    "#     dat = dict({\n",
    "#         'MessageGuid': msg.get('MessageGuid'),\n",
    "#         'CorrelationId': msg.get('CorrelationId'),\n",
    "#         'ApplicationMessageId': msg.get('ApplicationMessageId'),\n",
    "#         'ApplicationMessageType': msg.get('ApplicationMessageType'),\n",
    "#         'LogStart': msg.get('LogStart'),\n",
    "#         'LogEnd': msg.get('LogEnd'),\n",
    "#         'Sender': msg.get('Sender'),\n",
    "#         'Receiver': msg.get('Receiver'),\n",
    "#         'IntegrationFlowName': msg.get('IntegrationFlowName'),\n",
    "#         'Status': msg.get('Status'),\n",
    "#         'LogLevel': msg.get('LogLevel'),\n",
    "#         'CustomStatus': msg.get('CustomStatus'),\n",
    "#         'TransactionId': msg.get('TransactionId'),\n",
    "#         'PreviousComponentName': msg.get('PreviousComponentName') \n",
    "#       })\n",
    "#     return dat\n",
    "\n",
    "# def download_logs():\n",
    "#     skipamount = 50\n",
    "    \n",
    "#     requestcount = math.ceil(getCount()/skipamount)\n",
    "    \n",
    "#     pbar = tqdm(total=requestcount, leave=False)\n",
    "#     pbar.set_description(desc='Downloading log entries', refresh=True)\n",
    "    \n",
    "#     url = \"\"+\\\n",
    "#         \"$inlinecount=allpages&$format=json&$orderby=LogEnd%20desc&$top=\"+str(skipamount)+\"&$skip={}\"\n",
    "      \n",
    "#     def req_split(data):\n",
    "#         dat = processMsg(data)\n",
    "#         pbar.update(1)\n",
    "#         return dat\n",
    "    \n",
    "#     toplist = list(range(skipamount,requestcount,skipamount))\n",
    "#     print(toplist)\n",
    "    \n",
    "#     df = pd.DataFrame(columns=['MessageGuid','CorrelationId','ApplicationMessageId','ApplicationMessageType',\\\n",
    "#                                'LogStart','LogEnd','Sender','Receiver''IntegrationFlowName','Status''LogLevel',\\\n",
    "#                                'CustomStatus','TransactionId','PreviousComponentName'])\n",
    "#     for i in toplist:\n",
    "#         res = session.get(url.format(i))\n",
    "#         contents = res.content.decode('utf8')\n",
    "#         data = json.loads(contents)\n",
    "#         dat = data.get('d').get('results')\n",
    "#         with Pool(45) as p:\n",
    "#             pm = p.imap_unordered(req_split,dat)\n",
    "#             pm = [i for i in pm]\n",
    "#             df = df.append(pm, ignore_index=True, sort=False)\n",
    "        \n",
    "#     return df\n",
    "# df = download_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T13:10:37.659917Z",
     "start_time": "2019-06-17T13:10:37.643951Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T11:23:17.960676Z",
     "start_time": "2019-06-17T11:23:17.955961Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "time = 1558174980003 / 1000\n",
    "datetime.datetime.fromtimestamp(int(time)).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T06:30:47.404864Z",
     "start_time": "2019-06-18T06:30:47.163824Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs2019-06-17T12:12:00 to 2019-05-18T12:23:00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T06:30:59.573208Z",
     "start_time": "2019-06-18T06:30:47.787594Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T09:40:04.432261Z",
     "start_time": "2019-06-18T09:40:04.427114Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T11:26:14.296563Z",
     "start_time": "2019-06-17T11:25:27.446694Z"
    }
   },
   "outputs": [],
   "source": [
    "MessageGuidURL = \"\"\n",
    "def getMessageText():\n",
    "    \n",
    "    res = session.get(MessageGuidURL)\n",
    "    contents = res.content.decode('utf8')\n",
    "    data = json.loads(contents)\n",
    "    \n",
    "    requestcount = data.get('d').get('__count')\n",
    "    \n",
    "    pbar = tqdm(total=int(requestcount), leave=False)\n",
    "    pbar.set_description(desc='Downloading log entries text', refresh=True)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for i in data.get('d').get('results'):\n",
    "        GUID = i.get('MessageGuid')\n",
    "        res = session.get(MSG_DETAIL_URL + GUID)\n",
    "        contents = res.content.decode('utf8')\n",
    "        root = ET.fromstring(contents)\n",
    "        tag = {}\n",
    "        tag[\"GUID\"] = GUID\n",
    "        tag[\"MSG\"] = root.findtext(\".//mplData\")\n",
    "        df = df.append(tag, ignore_index=True)\n",
    "        pbar.update(1)\n",
    "    return df\n",
    "            \n",
    "msgdf = getMessageText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T11:24:33.474344Z",
     "start_time": "2019-06-17T11:23:43.896Z"
    }
   },
   "outputs": [],
   "source": [
    "msgdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T12:59:28.713250Z",
     "start_time": "2019-06-17T12:07:53.029495Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T12:59:30.144302Z",
     "start_time": "2019-06-17T12:59:28.819526Z"
    }
   },
   "outputs": [],
   "source": [
    "msgs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T13:00:03.545691Z",
     "start_time": "2019-06-17T12:59:39.663567Z"
    }
   },
   "outputs": [],
   "source": [
    "msgs.to_csv(\"log msg 2019-06-17T12:12:00 to 2019-05-18T12:23:00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T12:23:17.260531Z",
     "start_time": "2019-06-19T12:23:17.255530Z"
    }
   },
   "outputs": [],
   "source": [
    "date = readfromfile()\n",
    "date = parser.parse(str(date))\n",
    "date = date.isoformat()\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T12:26:27.165411Z",
     "start_time": "2019-06-19T12:26:27.113578Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"\"\n",
    "    \n",
    "data = request(200,200,url)\n",
    "\n",
    "count = data.get('d').get('__count')\n",
    "requestcount = math.ceil(int(count)/200)\n",
    "requestcount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T11:53:12.374405Z",
     "start_time": "2019-08-13T11:53:12.371776Z"
    }
   },
   "outputs": [],
   "source": [
    "url = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T11:53:13.051395Z",
     "start_time": "2019-08-13T11:53:13.042447Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "filteruri = '&$filter='\n",
    "def filterFill(interfacename):\n",
    "    return \"IntegrationFlowName ne '\"+interfacename+\"'\"\n",
    "\n",
    "def createFilteredURL(interfacenames, url):\n",
    "    count = 0\n",
    "    for i in interfacenames:\n",
    "        if count > 1:\n",
    "            url += \" and \" + filterFill(i)\n",
    "        elif count == 0:\n",
    "            url = url + filteruri + filterFill(i)\n",
    "        count += 1\n",
    "    return url\n",
    "    \n",
    "\n",
    "def getInterFaceList():\n",
    "    interfacelist = []\n",
    "    interfacecount = []\n",
    "    previouscount = \"\"\n",
    "    count = 2\n",
    "    initialload = True\n",
    "    while count > 1:\n",
    "        localurl = createFilteredURL(interfacelist, url)\n",
    "        res = session.get(localurl)\n",
    "        contents = res.content.decode('utf8')\n",
    "        data = json.loads(contents)\n",
    "        count = int(data.get('d').get('__count'))\n",
    "        print(count)\n",
    "        if initialload:\n",
    "            pbar = tqdm(total=count, leave=False)\n",
    "            pbar.set_description(desc='Downloading Interfaces', refresh=True)\n",
    "            initialload = False\n",
    "        else:\n",
    "            pbar.update(interfacecount[0]-count)\n",
    "        interfacecount.append(count)\n",
    "        if count > 1:\n",
    "            interface = data.get('d').get('results')[0]\n",
    "            interface = interface.get('IntegrationFlowName')\n",
    "            interfacelist.append(interface)\n",
    "    pbar.close()\n",
    "    return interfacelist, interfacecount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:19:40.607953Z",
     "start_time": "2019-08-13T11:53:13.614366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7104685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7104685), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6475713\n",
      "6475732\n",
      "5333517\n",
      "5287675\n",
      "5090478\n",
      "4695032\n",
      "4694412\n",
      "4600744\n",
      "4590758\n",
      "4568806\n",
      "4568423\n",
      "4477756\n",
      "4447325\n",
      "4430251\n",
      "4417438\n",
      "4127600\n",
      "3982615\n",
      "3726058\n",
      "3637489\n",
      "3634003\n",
      "3532279\n",
      "3495898\n",
      "3362691\n",
      "3221532\n",
      "3191107\n",
      "3172309\n",
      "3163647\n",
      "3158210\n",
      "3144119\n",
      "3127340\n",
      "3122687\n",
      "3053518\n",
      "3019476\n",
      "3015259\n",
      "3011313\n",
      "2996140\n",
      "2988442\n",
      "2974379\n",
      "2973026\n",
      "2934947\n",
      "2905334\n",
      "2875784\n",
      "2846448\n",
      "2836814\n",
      "2827181\n",
      "2778106\n",
      "2764978\n",
      "2755317\n",
      "2745666\n",
      "2716765\n",
      "2673419\n",
      "2662479\n",
      "2633480\n",
      "2604113\n",
      "2592517\n",
      "2573275\n",
      "2568187\n",
      "2538656\n",
      "2242677\n",
      "2064343\n",
      "2040661\n",
      "1279278\n",
      "1278924\n",
      "1276843\n",
      "1275830\n",
      "1274926\n",
      "1264471\n",
      "1263523\n",
      "1258447\n",
      "1257126\n",
      "1255379\n",
      "1254087\n",
      "1251657\n",
      "1251420\n",
      "1251289\n",
      "1222564\n",
      "1191487\n",
      "1188431\n",
      "1181436\n",
      "1174446\n",
      "1174349\n",
      "1170661\n",
      "1168609\n",
      "1167896\n",
      "1164836\n",
      "1164024\n",
      "1161051\n",
      "1160493\n",
      "1160043\n",
      "1159810\n",
      "1159730\n",
      "1158510\n",
      "1158432\n",
      "1150338\n",
      "1150267\n",
      "1150188\n",
      "1143198\n",
      "1131120\n",
      "1129116\n",
      "1128500\n",
      "1116986\n",
      "1109967\n",
      "1108491\n",
      "1107726\n",
      "1102709\n",
      "1097878\n",
      "1093047\n",
      "1088215\n",
      "1083383\n",
      "1082619\n",
      "1081853\n",
      "1081083\n",
      "1080153\n",
      "1079001\n",
      "1069503\n",
      "1045474\n",
      "1044197\n",
      "1043862\n",
      "878058\n",
      "798777\n",
      "796196\n",
      "789501\n",
      "782738\n",
      "774804\n",
      "774518\n",
      "774479\n",
      "774138\n",
      "773969\n",
      "771133\n",
      "766292\n",
      "763869\n",
      "758389\n",
      "755972\n",
      "750487\n",
      "747852\n",
      "741778\n",
      "739259\n",
      "733052\n",
      "731353\n",
      "728931\n",
      "727233\n",
      "723961\n",
      "721545\n",
      "719083\n",
      "716665\n",
      "714232\n",
      "711814\n",
      "709365\n",
      "702252\n",
      "696771\n",
      "695071\n",
      "692655\n",
      "690222\n",
      "687631\n",
      "687393\n",
      "686982\n",
      "683186\n",
      "683083\n",
      "675168\n",
      "665986\n",
      "665795\n",
      "650004\n",
      "649807\n",
      "649704\n",
      "649332\n",
      "649309\n",
      "648626\n",
      "648271\n",
      "648066\n",
      "647861\n",
      "447644\n",
      "447439\n",
      "447220\n",
      "447015\n",
      "445074\n",
      "444869\n",
      "444724\n",
      "444484\n",
      "444281\n",
      "444063\n",
      "444040\n",
      "442901\n",
      "442380\n",
      "442134\n",
      "441926\n",
      "441543\n",
      "441138\n",
      "440930\n",
      "440521\n",
      "438491\n",
      "438281\n",
      "437805\n",
      "437724\n",
      "437700\n",
      "437699\n",
      "437519\n",
      "437510\n",
      "437501\n",
      "437461\n",
      "437442\n",
      "437422\n",
      "437402\n",
      "437382\n",
      "437362\n",
      "437340\n",
      "437319\n",
      "437300\n",
      "437280\n",
      "437260\n",
      "437240\n",
      "437220\n",
      "437200\n",
      "437179\n",
      "437157\n",
      "437134\n",
      "437117\n",
      "437100\n",
      "437083\n",
      "437063\n",
      "436723\n",
      "436493\n",
      "436473\n",
      "436453\n",
      "436353\n",
      "436253\n",
      "286908\n",
      "286808\n",
      "286708\n",
      "286682\n",
      "286662\n",
      "286642\n",
      "286622\n",
      "286602\n",
      "286582\n",
      "286562\n",
      "286542\n",
      "286522\n",
      "286488\n",
      "286316\n",
      "286296\n",
      "286276\n",
      "286256\n",
      "286156\n",
      "286136\n",
      "284928\n",
      "274326\n",
      "274306\n",
      "274286\n",
      "274166\n",
      "272608\n",
      "272588\n",
      "272488\n",
      "258634\n",
      "203765\n",
      "203762\n",
      "164123\n",
      "164023\n",
      "163923\n",
      "163903\n",
      "163883\n",
      "163863\n",
      "163843\n",
      "163823\n",
      "163725\n",
      "163705\n",
      "163685\n",
      "163670\n",
      "163650\n",
      "163551\n",
      "163531\n",
      "163512\n",
      "163494\n",
      "163476\n",
      "163458\n",
      "163440\n",
      "163422\n",
      "95543\n",
      "95146\n",
      "94415\n",
      "94022\n",
      "93347\n",
      "93329\n",
      "93297\n",
      "93197\n",
      "93177\n",
      "93157\n",
      "93137\n",
      "93117\n",
      "93097\n",
      "93077\n",
      "92919\n",
      "92883\n",
      "92865\n",
      "92847\n",
      "92810\n",
      "92791\n",
      "92773\n",
      "92673\n",
      "92215\n",
      "92194\n",
      "92166\n",
      "92149\n",
      "92137\n",
      "92121\n",
      "88689\n",
      "88666\n",
      "83506\n",
      "83367\n",
      "72994\n",
      "72992\n",
      "72666\n",
      "72661\n",
      "66712\n",
      "60762\n",
      "60747\n",
      "60729\n",
      "60711\n",
      "60696\n",
      "32346\n",
      "3995\n",
      "3905\n",
      "3889\n",
      "3874\n",
      "3859\n",
      "3574\n",
      "3573\n",
      "3114\n",
      "1275\n",
      "825\n",
      "338\n",
      "186\n",
      "184\n",
      "158\n",
      "155\n",
      "154\n",
      "149\n",
      "145\n",
      "144\n",
      "53\n",
      "45\n",
      "43\n",
      "41\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "interfaceslist, countsremaining = getInterFaceList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:19:49.637037Z",
     "start_time": "2019-08-13T12:19:49.633440Z"
    }
   },
   "outputs": [],
   "source": [
    "actualcount = []\n",
    "for i in range(0,len(countsremaining)-1):\n",
    "    actualcount.append(countsremaining[i]-countsremaining[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:21:50.560901Z",
     "start_time": "2019-08-13T12:21:50.462514Z"
    }
   },
   "outputs": [],
   "source": [
    "interf_df = pd.DataFrame(\n",
    "    {'interfaces': interfaceslist,\n",
    "     'count': actualcount,\n",
    "     'remainingcount': countsremaining[:-1]\n",
    "    })\n",
    "interf_df.to_excel(\"exp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:20:49.440027Z",
     "start_time": "2019-08-13T12:20:49.436731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "344\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "print(len(interfaceslist))\n",
    "print(len(actualcount))\n",
    "print(len(countsremaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "380px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}